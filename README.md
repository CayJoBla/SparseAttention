# SparseAttention

Many of the recent advances in NLP have come largely due to the fact that the size of these models is simply getting larger. There are many models that work excellently, but need to be run on a configuration of 8+ high-end GPUs or TPUs, making it extremely difficult for the average consumer or developer to play around with these models and come up with new advances and ideas of their own. However, recent work, such as that seen in the [Longformer paper](https://arxiv.org/pdf/2004.05150.pdf), suggests that the attention mechanism that is present in these language models does not need to be dense, and can be employed with sparse matrix operations in order to increase the context length. Furthermore, signs point to the idea that the new [GPT 4 Turbo model](https://help.openai.com/en/articles/8555510-gpt-4-turbo) employs these sparse matrix operations in order to increase its context window. The attention mechanism employed by these transformer models has become the staple for NLP ever since the release of the ["Attention is All You Need"](https://arxiv.org/pdf/1706.03762.pdf) paper. However, with the introduction of working sparse attention models, a new question arises: **How much attention do we really need?**. In this project, I aim to use the `pytorch-block-sparse` package in order to implement this sparse attention mechanism into the existing base BERT model, following the sparsity pattern outlined in the Longformer paper. However, in contrast to that paper, by doing this I aim to decrease the overall size of the BERT model instead of trying to increase the model's context length.