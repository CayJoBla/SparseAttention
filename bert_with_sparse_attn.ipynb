{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> BERT with Sparse Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the recent advances in NLP have come largely due to the fact that the size of these models is simply getting larger. There are many models that work excellently, but need to be run on a configuration of 8+ high-end GPUs or TPUs, making it extremely difficult for the average consumer or developer to play around with these models and come up with new advances and ideas of their own. However, recent work, such as that seen in the [Longformer paper](https://arxiv.org/pdf/2004.05150.pdf), suggests that the attention mechanism that is present in these language models does not need to be dense, and can be employed with sparse matrix operations in order to increase the context length. Furthermore, signs point to the idea that the new [GPT 4 Turbo model](https://help.openai.com/en/articles/8555510-gpt-4-turbo) employs these sparse matrix operations in order to increase its context window. The attention mechanism employed by these transformer models has become the staple for NLP ever since the release of the [\"Attention is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf) paper. However, with the introduction of working sparse attention models, a new question arises: **How much attention do we really need?**. In this project, I aim to use the `pytorch-block-sparse` package in order to implement this sparse attention mechanism into the existing base BERT model, following the sparsity pattern outlined in the Longformer paper. However, in contrast to that paper, by doing this I aim to decrease the overall size of the BERT model instead of trying to increase the model's context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Dense-Sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try converting the attention layers of the BERT model into dense representations of the sparse attention pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the base BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert    # Look at the structure of the model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with a single layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first test converting a single attention layer (including the Q, K, and V weights) to sparse versions. Here, the tensors themselves are still dense, but weights that would be 0 in the true sparse matrix are set as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSelfAttention(\n",
       "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab the 0th attention mechanism\n",
    "attn0 = bert.encoder.layer[0].attention.self\n",
    "attn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0164,  0.0261, -0.0263,  ...,  0.0154,  0.0768,  0.0548],\n",
       "        [-0.0326,  0.0346, -0.0423,  ..., -0.0527,  0.1393,  0.0078],\n",
       "        [ 0.0105,  0.0334,  0.0109,  ..., -0.0279,  0.0258, -0.0468],\n",
       "        ...,\n",
       "        [-0.0085,  0.0514,  0.0555,  ...,  0.0282,  0.0543, -0.0541],\n",
       "        [-0.0198,  0.0944,  0.0617,  ..., -0.1042,  0.0601,  0.0470],\n",
       "        [ 0.0015, -0.0952,  0.0099,  ..., -0.0191, -0.0508, -0.0085]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q, K, V = attn0.query.weight.data, attn0.key.weight.data, attn0.value.weight.data\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the Sparsity Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on using the sliding window structure for the attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_sliding_window(weights, window_size=4):\n",
    "    \"\"\"\n",
    "    Keep only the weights within a specified window along the diagonals of the attention weight tensor.\n",
    "\n",
    "    Parameters:\n",
    "        weight (torch.Tensor): Attention weight tensor\n",
    "        window_size (int): Size of the window along the diagonals\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Sparse attention weight tensor with values outside the window set to 0\n",
    "    \"\"\"\n",
    "    assert weights.dim() == 2, \"Input tensor must be 2D\"\n",
    "    assert weights.size(0) == weights.size(1), \"Input tensor must be square\"\n",
    "    assert window_size % 2 == 0, \"Window size must be an even number\"\n",
    "\n",
    "    # Create mask\n",
    "    idx = torch.arange(weights.size(0))\n",
    "    diag_idx = idx.unsqueeze(0) - idx.unsqueeze(1)\n",
    "    mask = diag_idx.abs() < window_size//2 + 1\n",
    "\n",
    "    return weights * mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0164,  0.0261, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0326,  0.0346, -0.0423,  ..., -0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0334,  0.0109,  ..., -0.0000,  0.0000, -0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.0000,  0.0000,  ...,  0.0282,  0.0543, -0.0000],\n",
       "        [-0.0000,  0.0000,  0.0000,  ..., -0.1042,  0.0601,  0.0470],\n",
       "        [ 0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0508, -0.0085]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sliding_window(Q, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert All Attention Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert each attention layer of the base BERT model to a dense-sparse version using a sliding window sparsity. The weights are not truly sparse, we just set values to zero that do not fall along the sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_sparse_convert(model, sparsify_fn, **kwargs):\n",
    "    \"\"\"Replace the dense attention weights in a model with sparse ones.\"\"\"\n",
    "    for bert_layer in model.encoder.layer:\n",
    "        attn = bert_layer.attention.self\n",
    "        attn.query.weight.data = sparsify_fn(attn.query.weight.data, **kwargs)\n",
    "        attn.key.weight.data = sparsify_fn(attn.key.weight.data, **kwargs)\n",
    "        attn.value.weight.data = sparsify_fn(attn.value.weight.data, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 64\n",
    "sparse_bert = dense_sparse_convert(bert, get_sliding_window, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_bert     # Verify the integrity of the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0164,  0.0261, -0.0263,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0326,  0.0346, -0.0423,  ..., -0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0105,  0.0334,  0.0109,  ..., -0.0000,  0.0000, -0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.0000,  0.0000,  ...,  0.0282,  0.0543, -0.0541],\n",
       "        [-0.0000,  0.0000,  0.0000,  ..., -0.1042,  0.0601,  0.0470],\n",
       "        [ 0.0000, -0.0000,  0.0000,  ..., -0.0191, -0.0508, -0.0085]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_bert.encoder.layer[0].attention.self.query.weight.data  # The weights are sparsified successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Block Sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `pytorch-block-sparse` package, I convert the dense-sparse attention layers to truly sparse linear modules. The package already has a way to initialize a sparse linear layer from a dense one; however, it is meant to be used with fully dense layers and a provided density ratio, thus implementing sparsity in its own way. Here, I want to maintain the chosen sparsity pattern, essentially creating a block sparse matrix, where blocks of zeros are not stored. In order to do that, I rewrite some of the class itself, allowing it to call methods of the `BlockSparseMatrix` class and define where the sparse blocks lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_block_sparse import BlockSparseMatrix\n",
    "\n",
    "def get_blocks(weights, block_shape):\n",
    "    \"\"\"Returns the block indices for a sliding window pseudo-sparse weight matrix.\"\"\"\n",
    "    window_size = compute_window_size(weights)\n",
    "    half_window = window_size // 2\n",
    "\n",
    "    assert half_window % block_shape[0] == 0, \"Half window must be divisible by block shape\"\n",
    "    blocks_per_window = half_window // block_shape[0]\n",
    "\n",
    "    # Create block mask\n",
    "    X, Y = BlockSparseMatrix.blocks_count_(weights.size(), block_shape)\n",
    "    diag_idx = torch.arange(X).unsqueeze(0) - torch.arange(Y).unsqueeze(1)\n",
    "    return (diag_idx.abs() <= blocks_per_window).nonzero()\n",
    "\n",
    "def compute_window_size(weights):\n",
    "    \"\"\"Compute the window size for a psuedo-sparse weight matrix.\"\"\"\n",
    "    assert weights.dim() == 2, \"Input tensor must be 2D\"\n",
    "    assert weights.size(0) > 0 and weights.size(1) > 0, \"Input tensor must be non-empty\"\n",
    "    \n",
    "    return (weights[0].nonzero().max().item()) * 2\n",
    "\n",
    "def compute_density(sparse_weights):\n",
    "    \"\"\"Compute the density of a psuedo-sparse weight matrix.\"\"\"\n",
    "    total_size = sparse_weights.shape[0] * sparse_weights.shape[1]\n",
    "    sparse_size = sparse_weights.data.shape[0]  * sparse_weights.data.shape[1] \n",
    "    return sparse_size / total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_block_sparse import BlockSparseLinear, BlockSparseMatrix\n",
    "from pytorch_block_sparse.block_sparse_linear import BlockSparseLinearFunction\n",
    "from torch import nn\n",
    "\n",
    "class BlockSparseFromLinear(BlockSparseLinear):\n",
    "    \"\"\"Our implementation of the BlockSparseLinear layer. \n",
    "    Requires the external get_blocks and compute_window_size functions.\n",
    "    \"\"\"\n",
    "    OPTIMIZED_BLOCK_SIZE = 32\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        torch_nn_linear,        # Only use values from the linear layer (no in_size, out_size, bias, etc.)\n",
    "        verbose = False,\n",
    "        **kwargs                # Pass any additional arguments to the sparsify function\n",
    "    ):\n",
    "        super(BlockSparseLinear, self).__init__()\n",
    "        self.fn = BlockSparseLinearFunction.apply\n",
    "        self.verbose = verbose\n",
    "        self.block_shape = (self.OPTIMIZED_BLOCK_SIZE, self.OPTIMIZED_BLOCK_SIZE)   # Modify to always set block size to optimal (32x32)\n",
    "        self._optimized = (\n",
    "            self.block_shape[0] == self.OPTIMIZED_BLOCK_SIZE and self.block_shape[1] == self.OPTIMIZED_BLOCK_SIZE\n",
    "        )\n",
    "\n",
    "        # Modify to use values from the linear layer\n",
    "        in_features = torch_nn_linear.in_features\n",
    "        out_features = torch_nn_linear.out_features\n",
    "        bias = torch_nn_linear.bias is not None\n",
    "        weight = torch_nn_linear.weight.data\n",
    "\n",
    "        if in_features % self.block_shape[1] != 0:\n",
    "            raise Exception(\n",
    "                f\"BlockSparseLinear invalid block_shape={self.block_shape[1]}, should be divisor of {in_features}\"\n",
    "            )\n",
    "        if out_features % self.block_shape[0] != 0:\n",
    "            raise Exception(\n",
    "                f\"BlockSparseLinear invalid block_shape={self.block_shape[0]}, should be divisor of {out_features}\"\n",
    "            )\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Get the block indices and number of blocks\n",
    "        blocks = get_blocks(weight, self.block_shape)\n",
    "        self.block_count = len(blocks)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weight = BlockSparseMatrix.from_dense(weight, self.block_shape, self.block_count, blocks=blocks)\n",
    "\n",
    "        self.weight = weight\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features, device=\"cuda\"))\n",
    "            with torch.no_grad():\n",
    "                self.bias.copy_(torch_nn_linear.bias)\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0164,  0.0261, -0.0263,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0326,  0.0346, -0.0423,  ..., -0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0105,  0.0334,  0.0109,  ..., -0.0000,  0.0000, -0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.0000,  0.0000,  ...,  0.0282,  0.0543, -0.0541],\n",
       "        [-0.0000,  0.0000,  0.0000,  ..., -0.1042,  0.0601,  0.0470],\n",
       "        [ 0.0000, -0.0000,  0.0000,  ..., -0.0191, -0.0508, -0.0085]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_Q = sparse_bert.encoder.layer[0].attention.self.query\n",
    "sparse_Q.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockSparseFromLinear(\n",
       "  (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_block_sparse import BlockSparseLinear\n",
    "\n",
    "# Try on a single pseudo-sparse weight matrix\n",
    "sparse_Q_linear = BlockSparseFromLinear(torch_nn_linear=sparse_Q)\n",
    "sparse_Q_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity = 0.8784722222222222\n"
     ]
    }
   ],
   "source": [
    "print(\"Sparsity =\", 1-compute_density(sparse_Q_linear.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungdankblast/SparseAttention/.venv/lib/python3.10/site-packages/pytorch_block_sparse-0.1.2-py3.10-linux-x86_64.egg/pytorch_block_sparse/block_sparse.py:414: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n",
      "  out = torch.sparse.FloatTensor(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(sparse_Q.weight, sparse_Q_linear.weight.to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above outputs, we can see that we are now successfully storing the weight matrix using sparse block matrix form. With our window size of 64 and our block size of 32x32 (which is the optimal size defined by the package), we get 70 total blocks. This results in a sparsity of almost 88%. We will now go ahead and convert the rest of the attention layers in the BERT model to the sparse block matrix form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_convert(sparse_model, **kwargs):\n",
    "    \"\"\"Convert the psuedo-sparse attention weights in a model to block sparse ones.\"\"\"\n",
    "    for layer in sparse_model.encoder.layer:\n",
    "        attn = layer.attention.self\n",
    "        attn.query = BlockSparseFromLinear(attn.query, **kwargs)\n",
    "        attn.key = BlockSparseFromLinear(attn.key, **kwargs)\n",
    "        attn.value = BlockSparseFromLinear(attn.value, **kwargs)\n",
    "    return sparse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (key): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (value): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_sparse_bert = sparse_convert(sparse_bert)\n",
    "block_sparse_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I compare the base BERT model and the sparse one. I look at total sparsity and run time for a forward pass through the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the base BERT model\n",
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the dense model: 109482240\n",
      "Number of parameters in the sparse model: 90829056\n",
      "Total sparsity: 0.17037634597173024\n"
     ]
    }
   ],
   "source": [
    "bert_total_params = bert.num_parameters()\n",
    "block_sparse_bert_total_params = block_sparse_bert.num_parameters()\n",
    "print(\"Number of parameters in the dense model:\", bert_total_params)\n",
    "print(\"Number of parameters in the sparse model:\", block_sparse_bert_total_params)\n",
    "print(\"Total sparsity:\", 1 - block_sparse_bert_total_params / bert_total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, though our attention layers had a sparsity of almost 88%, since we only made the attention layers sparse, the sparsity of the total model is only around 17%, meaning that we have not significant reduced the size of the model. We could look into making the other layers of the model sparse, but it is unclear as to what type of sparsity pattern that we should use. Furthermore, the block sparse pattern that we are using now still has disadvantages, since we still end up saving a good number of zero values. A more optimal storage would be a diagonal sparse matrix representation, where diagonals and off-diagonals are stored as vectors. We compute the sparsity of that storage type below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume a window size of 64\n",
    "window_size = 64\n",
    "offsets = torch.arange(-window_size//2, window_size//2+1)\n",
    "sparse_diag_Q = [sparse_Q.weight.data.diag(off) for off in offsets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in a single attention layer in the dense model: 589824\n",
      "Number of parameters in a single attention layer in the sparse model: 48864\n",
      "Sparsity: 0.9171549479166666\n"
     ]
    }
   ],
   "source": [
    "# Compute sparsity\n",
    "sparse_diag_params = sum(len(d) for d in sparse_diag_Q)\n",
    "total_params = 768 * 768\n",
    "print(\"Number of parameters in a single attention layer in the dense model:\", total_params)\n",
    "print(\"Number of parameters in a single attention layer in the sparse model:\", sparse_diag_params)\n",
    "print(\"Sparsity:\", 1 - sparse_diag_params / total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, with a diagonal sparse matrix, we can get up to almost 92% sparsity in each attention layer. Unfortunately, we are storing the weights in a list here, and tensors have a more defined shape, and so we would likely need to store the diagonals with some zero values if we wanted to store them all on a single tensor for ease in forward computation. This structure also lends itself to sliding window attention patterns with a dilation factor, where cerain tokens are skipped and ignored, as seen in the Longformer paper. This kind of attention pattern could not be efficiently stored in a block-sparse form, as the resulting sparse attention matrix would have alternating diagonals of weights and zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using this optimized storage method would not change much in the number of overall parameters, and we would still have a sparsity of only around 18% on the total model. Therefore, in order to truly reduce the size of the model, we would likely have to sparsify other layers within the BERT model. In the following code block, I use the built-in `BlockSparseModelPatcher` of the `pytorch-block-sparse` package to sparsify other layers in BERT while maintaining the sparsity pattern we set previously for the attention layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse patching BERT on non-Attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching with BlockSparseLinear 'encoder.layer.0.attention.output.dense' with density=0.25, in=768, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.0.intermediate.dense' with density=0.25, in=768, out=3072,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.0.output.dense' with density=0.25, in=3072, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.1.attention.output.dense' with density=0.25, in=768, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.1.intermediate.dense' with density=0.25, in=768, out=3072,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.1.output.dense' with density=0.25, in=3072, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.2.attention.output.dense' with density=0.25, in=768, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.2.intermediate.dense' with density=0.25, in=768, out=3072,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.2.output.dense' with density=0.25, in=3072, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.3.attention.output.dense' with density=0.25, in=768, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.3.intermediate.dense' with density=0.25, in=768, out=3072,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.3.output.dense' with density=0.25, in=3072, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.4.attention.output.dense' with density=0.25, in=768, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.4.intermediate.dense' with density=0.25, in=768, out=3072,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.4.output.dense' with density=0.25, in=3072, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.5.attention.output.dense' with density=0.25, in=768, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.5.intermediate.dense' with density=0.25, in=768, out=3072,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.5.output.dense' with density=0.25, in=3072, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.6.attention.output.dense' with density=0.25, in=768, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.6.intermediate.dense' with density=0.25, in=768, out=3072,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.6.output.dense' with density=0.25, in=3072, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.7.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.7.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.7.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.8.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.8.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.8.output.dense' with density=0.5, in=3072, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.9.attention.output.dense' with density=0.5, in=768, out=768,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.9.intermediate.dense' with density=0.5, in=768, out=3072,bias=True \n",
      "Patching with BlockSparseLinear 'encoder.layer.9.output.dense' with density=0.5, in=3072, out=768,bias=True \n"
     ]
    }
   ],
   "source": [
    "from pytorch_block_sparse import BlockSparseModelPatcher\n",
    "\n",
    "# Re-initialize sparse BERT\n",
    "full_sparse_bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "full_sparse_bert = sparse_convert(dense_sparse_convert(full_sparse_bert, get_sliding_window, window_size=64))\n",
    "\n",
    "# Create a model patcher\n",
    "mp = BlockSparseModelPatcher()\n",
    "\n",
    "# Use increasing density as the layer index increases\n",
    "mp.add_pattern(\"encoder\\.layer\\.[0-6]\\.intermediate\\.dense\", {\"density\":0.25})\n",
    "mp.add_pattern(\"encoder\\.layer\\.[0-6]\\.output\\.dense\", {\"density\":0.25})\n",
    "mp.add_pattern(\"encoder\\.layer\\.[0-6]\\.attention\\.output\\.dense\", {\"density\":0.25})\n",
    "mp.add_pattern(\"encoder\\.layer\\.[7-9]+\\.intermediate\\.dense\", {\"density\":0.5})\n",
    "mp.add_pattern(\"encoder\\.layer\\.[7-9]+\\.output\\.dense\", {\"density\":0.5})\n",
    "mp.add_pattern(\"encoder\\.layer\\.[7-9]+\\.attention\\.output\\.dense\", {\"density\":0.5})\n",
    "mp.patch_model(full_sparse_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-6): 7 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (key): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (value): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): BlockSparseLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([144, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([4608, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): BlockSparseLinear(\n",
       "            (weight): BlockSparseMatrix(shape=torch.Size([3072, 768]), cols=torch.Size([576, 2]), row_start_ends_a=torch.Size([97]), data=torch.Size([18432, 32]), block_shape=(32, 32))\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): BlockSparseLinear(\n",
       "            (weight): BlockSparseMatrix(shape=torch.Size([768, 3072]), cols=torch.Size([576, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([18432, 32]), block_shape=(32, 32))\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7-9): 3 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (key): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (value): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): BlockSparseLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([288, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([9216, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): BlockSparseLinear(\n",
       "            (weight): BlockSparseMatrix(shape=torch.Size([3072, 768]), cols=torch.Size([1152, 2]), row_start_ends_a=torch.Size([97]), data=torch.Size([36864, 32]), block_shape=(32, 32))\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): BlockSparseLinear(\n",
       "            (weight): BlockSparseMatrix(shape=torch.Size([768, 3072]), cols=torch.Size([1152, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([36864, 32]), block_shape=(32, 32))\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (key): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (value): BlockSparseFromLinear(\n",
       "              (weight): BlockSparseMatrix(shape=torch.Size([768, 768]), cols=torch.Size([70, 2]), row_start_ends_a=torch.Size([25]), data=torch.Size([2240, 32]), block_shape=(32, 32))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_sparse_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model parameters count=54997248\n",
      "Final model sparsity=0.49766055206762305\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final model parameters count={full_sparse_bert.num_parameters()}\")\n",
    "print(f\"Final model sparsity={1 - full_sparse_bert.num_parameters() / bert_total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by adding some sparsity to other linear layers within the encoder part of the model, we were able to get the total model sparsity to almost 50%. Now, unfortunately, I have no way of figuring out whether I made the layers too sparse, or whether they could be more sparse and still retain some of their performance. Though I was unable to implement them here, I did see some preliminary research online into models that can learn sparsity, choosing how sparse to make themselves as they are trained. It would be worth looking into these types of models for the other linear layers in the encoder structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151fe96d2d644095bc6a23361a56a51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd13275e55649b2b84ad9fa13f2d38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize a data subset for testing (This data has been preprocessed and tokenized already)\n",
    "wikipedia = load_dataset(\"cayjobla/wikipedia-pretrain-processed\", split=\"train[:10000]\")\n",
    "wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\")   # Reinitialize the dense model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the time it takes for a forward pass of the model, I run multiple batches from the training data on each of the models and analyze the computation times of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def batch_data(data, batch_size=8):\n",
    "    \"\"\"Batch the data into batches of the specified size.\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]\n",
    "\n",
    "def run_test(model, data, batch_size=16, device=\"cuda\"):\n",
    "    \"\"\"Run a forward pass on the model with the given data.\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    times = []\n",
    "    for batch in batch_data(data, batch_size=batch_size):\n",
    "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "        token_type_ids = torch.tensor(batch[\"token_type_ids\"]).to(device)\n",
    "    \n",
    "        start = time.time()\n",
    "        model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    model.to(\"cpu\")\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "bert_time = run_test(bert, wikipedia, batch_size=16, device=\"cuda\")\n",
    "plt.hist(bert_time, bins=15)\n",
    "plt.title(f\"Average time taken by the dense BERT model: {round(np.mean(bert_time), 4)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sparse_bert_time = run_test(sparse_bert, wikipedia, batch_size=16, device=\"cuda\")\n",
    "plt.hist(sparse_bert_time, bins=15)\n",
    "plt.title(f\"Average time taken by the sparse attention BERT model: {round(np.mean(sparse_bert_time), 4)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_sparse_bert_time = run_test(full_sparse_bert, wikipedia, batch_size=16, device=\"cuda\")\n",
    "plt.hist(full_sparse_bert_time, bins=15)\n",
    "plt.title(f\"Average time taken by the full sparse BERT model: {round(np.mean(full_sparse_bert_time), 4)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, it appears that the more sparse that a model is, the longer that it takes on average to run a batch of 16 training examples through the model. This is interesting, since we would expect the more sparse models to run faster since they are performing fewer FLOPS for each forward run through the model. Likely, this discrepancy is a result of the fact that dense tensor operations have been optimized over many years of use; whereas sparse matrix operations on GPUs are still being optimized and developed currently. Thus, while more total FLOPs are being done under the dense model, perhaps those FLOPs are much easier to parallelize on the GPU when compared to the sparse matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the Sparse Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I attempt to fine-tune the sparse model using the COLA task data from the GLUE benchmark dataset. This way, I can compare the results of the sparse model to the base BERT model, and determine how much performance is lost when making the attention mechanism sparse. In order to fine-tune the model, I use a modified version of a script that I have written for a different project. The script used here is called `finetune_sparse_bert.py`, and it can be found at my [Github repo](https://github.com/CayJoBla/SparseAttention) for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the 'cola' split of the 'glue' dataset...\n",
      "Loading bert-base-uncased tokenizer...\n",
      "Loading bert-base-uncased model...\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Sparsifying bert-base-uncased model...\n",
      "Defining training arguments...\n",
      "Preprocessing the dataset for the 'cola' task...\n",
      "Running tokenizer on dataset: 100%|| 1043/1043 [00:00<00:00, 14668.25 examples/\n",
      "Load the evaluation metric for the 'cola' task...\n",
      "Initialize Trainer...\n",
      "Train the model...\n",
      "  0%|                                          | 1/1605 [00:00<12:01,  2.22it/s]\n",
      "  0%|                                                    | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|                                        | 5/66 [00:00<00:01, 47.51it/s]\u001b[A\n",
      " 15%|                                    | 10/66 [00:00<00:01, 41.52it/s]\u001b[A\n",
      " 23%|                                 | 15/66 [00:00<00:01, 39.86it/s]\u001b[A\n",
      " 30%|                              | 20/66 [00:00<00:01, 39.29it/s]\u001b[A\n",
      " 36%|                           | 24/66 [00:00<00:01, 39.00it/s]\u001b[A\n",
      " 42%|                        | 28/66 [00:00<00:00, 38.64it/s]\u001b[A\n",
      " 48%|                      | 32/66 [00:00<00:00, 38.42it/s]\u001b[A\n",
      " 55%|                   | 36/66 [00:00<00:00, 38.17it/s]\u001b[A\n",
      " 61%|                 | 40/66 [00:01<00:00, 38.02it/s]\u001b[A\n",
      " 67%|              | 44/66 [00:01<00:00, 38.08it/s]\u001b[A\n",
      " 73%|           | 48/66 [00:01<00:00, 38.04it/s]\u001b[A\n",
      " 79%|         | 52/66 [00:01<00:00, 38.08it/s]\u001b[A\n",
      " 85%|      | 56/66 [00:01<00:00, 38.17it/s]\u001b[A\n",
      " 91%|    | 60/66 [00:01<00:00, 38.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6782962679862976, 'eval_matthews_correlation': 0.0, 'eval_runtime': 1.723, 'eval_samples_per_second': 605.333, 'eval_steps_per_second': 38.305, 'epoch': 0.0}\n",
      "  0%|                                          | 1/1605 [00:02<12:01,  2.22it/s]\n",
      "100%|| 66/66 [00:01<00:00, 37.99it/s]\u001b[A\n",
      "{'loss': 0.6303, 'learning_rate': 2.8130841121495327e-05, 'epoch': 0.19}        \u001b[A\n",
      "{'loss': 0.609, 'learning_rate': 2.6261682242990656e-05, 'epoch': 0.37}         \n",
      "{'loss': 0.6025, 'learning_rate': 2.4392523364485982e-05, 'epoch': 0.56}        \n",
      "{'loss': 0.602, 'learning_rate': 2.2523364485981308e-05, 'epoch': 0.75}         \n",
      "{'loss': 0.6115, 'learning_rate': 2.0654205607476634e-05, 'epoch': 0.93}        \n",
      " 33%|                          | 534/1605 [00:47<01:30, 11.83it/s]\n",
      "  0%|                                                    | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|                                        | 5/66 [00:00<00:01, 48.03it/s]\u001b[A\n",
      " 15%|                                    | 10/66 [00:00<00:01, 41.99it/s]\u001b[A\n",
      " 23%|                                 | 15/66 [00:00<00:01, 40.18it/s]\u001b[A\n",
      " 30%|                              | 20/66 [00:00<00:01, 39.56it/s]\u001b[A\n",
      " 36%|                           | 24/66 [00:00<00:01, 39.01it/s]\u001b[A\n",
      " 42%|                        | 28/66 [00:00<00:00, 38.62it/s]\u001b[A\n",
      " 48%|                      | 32/66 [00:00<00:00, 38.15it/s]\u001b[A\n",
      " 55%|                   | 36/66 [00:00<00:00, 38.23it/s]\u001b[A\n",
      " 61%|                 | 40/66 [00:01<00:00, 38.43it/s]\u001b[A\n",
      " 67%|              | 44/66 [00:01<00:00, 38.11it/s]\u001b[A\n",
      " 73%|           | 48/66 [00:01<00:00, 38.07it/s]\u001b[A\n",
      " 79%|         | 52/66 [00:01<00:00, 38.25it/s]\u001b[A\n",
      " 85%|      | 56/66 [00:01<00:00, 38.47it/s]\u001b[A\n",
      " 91%|    | 60/66 [00:01<00:00, 38.47it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6147463917732239, 'eval_matthews_correlation': 0.0, 'eval_runtime': 1.7169, 'eval_samples_per_second': 607.479, 'eval_steps_per_second': 38.441, 'epoch': 1.0}\n",
      " 33%|                          | 535/1605 [00:49<01:30, 11.83it/s]\n",
      "100%|| 66/66 [00:01<00:00, 38.26it/s]\u001b[A\n",
      "{'loss': 0.6129, 'learning_rate': 1.8785046728971963e-05, 'epoch': 1.12}        \u001b[A\n",
      "{'loss': 0.6091, 'learning_rate': 1.691588785046729e-05, 'epoch': 1.31}         \n",
      "{'loss': 0.5986, 'learning_rate': 1.5046728971962615e-05, 'epoch': 1.5}         \n",
      "{'loss': 0.5924, 'learning_rate': 1.3177570093457945e-05, 'epoch': 1.68}        \n",
      "{'loss': 0.5775, 'learning_rate': 1.130841121495327e-05, 'epoch': 1.87}         \n",
      " 67%|             | 1070/1605 [01:34<00:42, 12.64it/s]\n",
      "  0%|                                                    | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|                                        | 5/66 [00:00<00:01, 47.49it/s]\u001b[A\n",
      " 15%|                                    | 10/66 [00:00<00:01, 41.77it/s]\u001b[A\n",
      " 23%|                                 | 15/66 [00:00<00:01, 39.88it/s]\u001b[A\n",
      " 30%|                              | 20/66 [00:00<00:01, 39.27it/s]\u001b[A\n",
      " 36%|                           | 24/66 [00:00<00:01, 38.81it/s]\u001b[A\n",
      " 42%|                        | 28/66 [00:00<00:00, 38.81it/s]\u001b[A\n",
      " 48%|                      | 32/66 [00:00<00:00, 38.65it/s]\u001b[A\n",
      " 55%|                   | 36/66 [00:00<00:00, 38.33it/s]\u001b[A\n",
      " 61%|                 | 40/66 [00:01<00:00, 38.14it/s]\u001b[A\n",
      " 67%|              | 44/66 [00:01<00:00, 38.20it/s]\u001b[A\n",
      " 73%|           | 48/66 [00:01<00:00, 38.33it/s]\u001b[A\n",
      " 79%|         | 52/66 [00:01<00:00, 37.81it/s]\u001b[A\n",
      " 85%|      | 56/66 [00:01<00:00, 37.87it/s]\u001b[A\n",
      " 91%|    | 60/66 [00:01<00:00, 37.83it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6063240766525269, 'eval_matthews_correlation': 0.047280793684450845, 'eval_runtime': 1.7246, 'eval_samples_per_second': 604.763, 'eval_steps_per_second': 38.269, 'epoch': 2.0}\n",
      " 67%|             | 1070/1605 [01:36<00:42, 12.64it/s]\n",
      "100%|| 66/66 [00:01<00:00, 38.00it/s]\u001b[A\n",
      "{'loss': 0.5839, 'learning_rate': 9.439252336448598e-06, 'epoch': 2.06}         \u001b[A\n",
      "{'loss': 0.5515, 'learning_rate': 7.570093457943924e-06, 'epoch': 2.24}         \n",
      "{'loss': 0.5445, 'learning_rate': 5.700934579439252e-06, 'epoch': 2.43}         \n",
      "{'loss': 0.5215, 'learning_rate': 3.831775700934579e-06, 'epoch': 2.62}         \n",
      "{'loss': 0.5269, 'learning_rate': 1.9626168224299064e-06, 'epoch': 2.8}         \n",
      "{'loss': 0.525, 'learning_rate': 9.345794392523364e-08, 'epoch': 2.99}          \n",
      "100%|| 1604/1605 [02:21<00:00, 11.88it/s]\n",
      "  0%|                                                    | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|                                        | 5/66 [00:00<00:01, 47.01it/s]\u001b[A\n",
      " 15%|                                    | 10/66 [00:00<00:01, 41.16it/s]\u001b[A\n",
      " 23%|                                 | 15/66 [00:00<00:01, 39.72it/s]\u001b[A\n",
      " 30%|                              | 20/66 [00:00<00:01, 39.20it/s]\u001b[A\n",
      " 36%|                           | 24/66 [00:00<00:01, 38.71it/s]\u001b[A\n",
      " 42%|                        | 28/66 [00:00<00:00, 38.64it/s]\u001b[A\n",
      " 48%|                      | 32/66 [00:00<00:00, 38.61it/s]\u001b[A\n",
      " 55%|                   | 36/66 [00:00<00:00, 38.60it/s]\u001b[A\n",
      " 61%|                 | 40/66 [00:01<00:00, 38.49it/s]\u001b[A\n",
      " 67%|              | 44/66 [00:01<00:00, 38.04it/s]\u001b[A\n",
      " 73%|           | 48/66 [00:01<00:00, 37.95it/s]\u001b[A\n",
      " 79%|         | 52/66 [00:01<00:00, 38.20it/s]\u001b[A\n",
      " 85%|      | 56/66 [00:01<00:00, 38.15it/s]\u001b[A\n",
      " 91%|    | 60/66 [00:01<00:00, 38.31it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6338092088699341, 'eval_matthews_correlation': 0.09739040474377686, 'eval_runtime': 1.7276, 'eval_samples_per_second': 603.726, 'eval_steps_per_second': 38.203, 'epoch': 3.0}\n",
      "100%|| 1605/1605 [02:23<00:00, 11.88it/s]\n",
      "100%|| 66/66 [00:01<00:00, 38.37it/s]\u001b[A\n",
      "{'train_runtime': 143.7737, 'train_samples_per_second': 178.426, 'train_steps_per_second': 11.163, 'train_loss': 0.5809513051933217, 'epoch': 3.0}\n",
      "100%|| 1605/1605 [02:23<00:00, 11.16it/s]\n",
      "Save the model...\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =      0.581\n",
      "  train_runtime            = 0:02:23.77\n",
      "  train_samples_per_second =    178.426\n",
      "  train_steps_per_second   =     11.163\n",
      "Evaluate the model on the validation set...\n",
      "100%|| 66/66 [00:01<00:00, 38.82it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                     =        3.0\n",
      "  eval_loss                 =     0.6338\n",
      "  eval_matthews_correlation =     0.0974\n",
      "  eval_runtime              = 0:00:01.72\n",
      "  eval_samples              =       1043\n",
      "  eval_samples_per_second   =    603.624\n",
      "  eval_steps_per_second     =     38.197\n",
      "Predict the test set labels...\n",
      "100%|| 67/67 [00:01<00:00, 39.32it/s]\n"
     ]
    }
   ],
   "source": [
    "!python finetune_sparse_bert.py \\\n",
    "  --tokenizer bert-base-uncased \\\n",
    "  --task cola \\\n",
    "  --batch_size 16 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_epochs 3 \\\n",
    "  --logging_steps 100 \\\n",
    "  --output_dir bert-base-uncased-sparse-64 \\\n",
    "  --run_name sparse-glue-cola \\\n",
    "  --seed 916 \\\n",
    "  -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training above, we got a matthew's correlation (the metric for this task) of 0.0974. Unfortunately, according to the [GLUE leaderboard](https://gluebenchmark.com/leaderboard) the matthew's correlation for the base BERT model is generally between 0.55 and 0.6, so we are severely underperforming with this sparse model. Therefore, it appears that the sparse attention model on this context length does not give us great performance on the BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results, Analysis, and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we took the base BERT model and applied a sliding window sparsity pattern to the attention layers of the model with the goal of reducing the size of the model instead of increasing the model's context length. First, we applied the sparsity pattern to the dense linear attention layers, keeping the remaining weights as they are found in the base BERT model. Next, from the psuedo-sparse dense layers, we created true block-sparse linear layers using a modified version of the `BlockSparseLinear` module found in the `pytorch-block-sparse` package. Comparing the resulting sparse attention BERT model to the base BERT model, we found that despite the fact that our attention layers had a sparsity of around 88%, the entire sparse attention model only had a sparsity of around 17% due to the existence of many larger dense linear layers within the model. Because of this, we decided to try using the `BlockSparseModelPatcher` from the `pytorch-block-sparse` package to apply random block sparsity to the other dense BERT model layers. Though we cannot be sure of the performance of the fully sparse model, we applied decreasing sparsity to the successive encoder layers (25% -> 50% -> 100% density), reaching a total model sparsity of around 50%. Next, we compared the run times of the different models (the dense BERT model, the sparse attention BERT model, and the fully sparse BERT model), and found that--in contrast to our expectations--the sparser models on average took longer to do a forward pass with a batch of 16 training samples than the denser models. After running these comparisons of sparsity and run time, we finally decided to compare the performance of the sparse attention BERT model to the base BERT model on the COLA task from the GLUE benchmark dataset. After fine-tuning for a few epochs on this task, we found that the sparse attention model performed significantly worse than the dense model (according to the matthew's correlation metric), getting a score of 0.09 while the base BERT model generally scores around 0.55-0.60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All-in-all, it seems that sparse attention layers as they currently stand are not an efficient way to reduce the size of a model while maintaining its performance. There are a variety of reasons as to why that may be:\n",
    "1. The attention sparcity pattern that we employed only used a sliding window, and it did not take into account dilation in the sliding windows or the use of global attention for certain tokens, as was used in the Longformer paper. The reason for this is that the block-sparse matrix implementation that we decided to use would not work well with these sparsity patterns, and would result in blocks within the attention mechanism that were themselves psuedo-sparse (densely-stored blocks, but with lots of zeros).\n",
    "2. The sparse matrix operations are not as efficient as those for dense matrices. At the point of writing this conclusion, research is being done into a variety of different implementations of sparse matrix operations within CUDA; however, many of them still suffer from inefficiencies. Recently, NVIDIA has released multiple articles, such as [this one](https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/), about recent advances with block-sparse matrix operations on NVIDIA GPUs; however, not much work has been done yet on getting those to work well with Pytorch. In fact, the provided article recommends the package that we used as the Pytorch implementation of block-sparse matrix operations, depite the fact that the package was last updated in 2020.\n",
    "3. While the block-sparse matrix format is probably the best sparse implementation for large networks, it is not the best way to store sparse matrices for sliding-window attention. Since the sliding window sparse attention pattern deals with diagonals, it would be better suited to a form of sparse matrix that stores the diagonal and off-diagonals of the layer as vectors. This way, no unnecessary values are stored, and sparsity can be maximized in those layers. Unfortunately, because this is not a common sparse matrix type, it likely will not get a ton of research, and so the sparse matrix operations for this form will not be very efficient either.\n",
    "4. Perhaps the sparse attention model did not perform well because it needed more training time. Here, we only fine-tuned the model, since it was initialized with the weights from the base BERT model, but it is possible that the sparse model needs enough data to again pretrain the model since the model format has changed so drastically.\n",
    "5. While the sparse attention form worked well for the long context models in the Longformer paper and (potentially) GPT 4 Turbo, it is possible that the sparse attention pattern is not suited for smaller context sizes, and long context models only benefit from the sparsity *because* of their longer context. It is possible that for shorter context lengths, more attention needs to be placed on the tokens so that the meaning can be better extracted from the smaller context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more time and resources, I would look at first implementing global attention alongside the sliding window attention, as seen in the Longformer document, and train the sparse model for a much longer time to determine whether the performance of the model can be improved to the level of the base BERT model. If significant improvement can be made, more research should be done into diagonal-sparse matrices and optimizing the sparse matrix operations for those matrices on the GPU so that models can be sparsified while maintaining or decreasing run time. Furthermore, while doing this project, I came across a few WIP implementations of linear layers for these models that can learn sparsity. If this idea appears to work decently well, some more research should be done into this idea so that other non-attention layers of these language models can be made sparse without drastically impacting the overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
